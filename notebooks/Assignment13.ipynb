{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: D:\\Projects_Tegebly_Sho8l_b3oon_allah\\Churn_Predictor\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Set this to your workspace root\n",
    "os.chdir(r\"D:\\Projects_Tegebly_Sho8l_b3oon_allah\\Churn_Predictor\")\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "def split_jsonl_file(input_path, train_path, test_path, test_ratio=0.2):\n",
    "    with open(input_path, 'r', encoding='utf-8') as infile, \\\n",
    "         open(train_path, 'w', encoding='utf-8') as train_file, \\\n",
    "         open(test_path, 'w', encoding='utf-8') as test_file:\n",
    "        \n",
    "        for line in infile:\n",
    "            if random.random() < test_ratio:\n",
    "                test_file.write(line)\n",
    "            else:\n",
    "                train_file.write(line)\n",
    "\n",
    "# Example\n",
    "split_jsonl_file(\"data/game2_processed/playerLogs_game2_playerbasedlines.jsonl\", \"data/game2_processed/train.jsonl\", \"data/game2_processed/test.jsonl\", test_ratio=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract features and prepare model dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def parse_datetime(dt_str):\n",
    "    \"\"\"\n",
    "    Convert ISO format datetime string to datetime object.\n",
    "    \n",
    "    Args:\n",
    "        dt_str (str): Datetime string in ISO format with 'Z' timezone marker\n",
    "        \n",
    "    Returns:\n",
    "        datetime: Parsed datetime object with UTC timezone\n",
    "    \"\"\"\n",
    "    return datetime.fromisoformat(dt_str.replace(\"Z\", \"+00:00\"))\n",
    "\n",
    "def extract_features(events, uid):\n",
    "    \"\"\"\n",
    "    Extract features from a user's event history for churn prediction.\n",
    "    \n",
    "    The function splits the data into two periods:\n",
    "    1. Observation period: First 5 days of user activity\n",
    "    2. Prediction period: Following 10 days\n",
    "    \n",
    "    Args:\n",
    "        events (list): List of event dictionaries containing user activity data\n",
    "        uid (str): User identifier\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing extracted features, or None if no valid events\n",
    "        \n",
    "    Features extracted:\n",
    "        - play_count: Total number of events in observation period\n",
    "        - active_days: Number of unique days with activity\n",
    "        - total_reward: Sum of all rewards earned\n",
    "        - mean_reward: Average reward per event\n",
    "        - reward_std: Standard deviation of rewards\n",
    "        - days_since_last_play: Days between last activity and observation end\n",
    "        - avg_session_gap: Average hours between consecutive events\n",
    "        - last_7_days_activity: Binary indicator of activity in last week\n",
    "        - churn: Binary indicator of user churning in prediction period\n",
    "    \"\"\"\n",
    "    if not events:\n",
    "        return None\n",
    "\n",
    "    # Sort events chronologically and define time windows\n",
    "    events = sorted(events, key=lambda x: parse_datetime(x['date']))\n",
    "    start_time = parse_datetime(events[0]['date'])\n",
    "    obs_end = start_time + timedelta(days=5)    # End of observation period\n",
    "    pred_end = obs_end + timedelta(days=10)     # End of prediction period\n",
    "\n",
    "    # Split events into observation and prediction periods\n",
    "    obs_events = [e for e in events if parse_datetime(e['date']) < obs_end]\n",
    "    pred_events = [e for e in events if obs_end <= parse_datetime(e['date']) < pred_end]\n",
    "\n",
    "    if not obs_events:\n",
    "        return None\n",
    "\n",
    "    # Extract basic event information\n",
    "    times = [parse_datetime(e['date']) for e in obs_events]\n",
    "    actions = [e.get('event', '') for e in obs_events]\n",
    "    rewards = [e.get('reward', 0) for e in obs_events]\n",
    "\n",
    "    # Calculate activity patterns\n",
    "    active_days = {t.date() for t in times}  # Set of unique active days\n",
    "    # Calculate time gaps between consecutive events (in hours)\n",
    "    gaps = [(times[i+1] - times[i]).total_seconds() / 3600 for i in range(len(times)-1)]\n",
    "\n",
    "    # Construct feature dictionary\n",
    "    return {\n",
    "        'uid': uid,\n",
    "        'play_count': len(obs_events),          # Total number of events\n",
    "        'active_days': len(active_days),        # Number of unique active days\n",
    "        'mean_score': np.mean(rewards),        # Average reward per event\n",
    "        'score_std': np.std(rewards), \n",
    "        'best_score': max(rewards),         # Reward variability\n",
    "        'days_since_last_play': (obs_end - max(times)).days,  # Recency of last activity\n",
    "        'avg_session_gap': np.mean(gaps) if gaps else 0,      # Average time between events\n",
    "        'last_7_days_activity': int(any((obs_end - t).days <= 7 for t in times)),  # Recent activity indicator\n",
    "        'churn': (len(pred_events) == 0)     # Churn label (1 if no activity in prediction period)\n",
    "    }\n",
    "\n",
    "def stream_features_to_csv_train(jsonl_path, output_csv):\n",
    "    \"\"\"\n",
    "    Process a JSONL file of user events and write extracted features to CSV.\n",
    "    \n",
    "    This function processes the input file line by line to handle large files\n",
    "    efficiently. For each user, it flattens their event records and extracts\n",
    "    features for churn prediction.\n",
    "    \n",
    "    Args:\n",
    "        jsonl_path (str): Path to input JSONL file containing user events\n",
    "        output_csv (str): Path where the output CSV file will be written\n",
    "        \n",
    "    Side effects:\n",
    "        - Creates or overwrites the output CSV file\n",
    "        - Prints progress messages every 10,000 lines\n",
    "        - Prints error messages for skipped lines\n",
    "    \"\"\"\n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as infile, \\\n",
    "         open(output_csv, 'w', encoding='utf-8', newline='') as outfile:\n",
    "\n",
    "        writer = None\n",
    "        for i, line in enumerate(infile):\n",
    "            try:\n",
    "                # Parse each line as a user record\n",
    "                row = json.loads(line)\n",
    "                uid = row.get('uid')\n",
    "                records = row.get('records', [])\n",
    "                \n",
    "                # Flatten and normalize event records\n",
    "                flat_events = []\n",
    "                for event in records:\n",
    "                    props = event.get('properties') or {}\n",
    "                    flat_events.append({\n",
    "                        'date': event.get('date'),\n",
    "                        'event': event.get('event', ''),\n",
    "                        # Extract numeric properties with safe fallbacks\n",
    "                        'reward': props.get('reward', 0) if isinstance(props, dict) else 0,\n",
    "                        'package': props.get('package', 0) if isinstance(props, dict) else 0\n",
    "                    })\n",
    "\n",
    "                # Extract features and write to CSV\n",
    "                features = extract_features(flat_events, uid)\n",
    "                if features:\n",
    "                    # Initialize CSV writer with headers on first valid record\n",
    "                    if writer is None:\n",
    "                        writer = csv.DictWriter(outfile, fieldnames=list(features.keys()))\n",
    "                        writer.writeheader()\n",
    "                    writer.writerow(features)\n",
    "\n",
    "                # Print progress update every 10,000 records\n",
    "                if (i + 1) % 10000 == 0:\n",
    "                    print(f\"Processed {i+1} lines...\")\n",
    "\n",
    "                if (i + 1) % 20000 == 0:\n",
    "                    break\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping line {i} due to error: {e}\")\n",
    "                continue\n",
    "\n",
    "    print(f\"âœ… Finished writing features to {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def stream_features_to_csv_test(jsonl_path, output_csv):\n",
    "    \"\"\"\n",
    "    Process a JSONL file of user events and write extracted features to CSV.\n",
    "    \n",
    "    This function processes the input file line by line to handle large files\n",
    "    efficiently. For each user, it flattens their event records and extracts\n",
    "    features for churn prediction.\n",
    "    \n",
    "    Args:\n",
    "        jsonl_path (str): Path to input JSONL file containing user events\n",
    "        output_csv (str): Path where the output CSV file will be written\n",
    "        \n",
    "    Side effects:\n",
    "        - Creates or overwrites the output CSV file\n",
    "        - Prints progress messages every 10,000 lines\n",
    "        - Prints error messages for skipped lines\n",
    "    \"\"\"\n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as infile, \\\n",
    "         open(output_csv, 'w', encoding='utf-8', newline='') as outfile:\n",
    "\n",
    "        writer = None\n",
    "        for i, line in enumerate(infile):\n",
    "            try:\n",
    "                # Parse each line as a user record\n",
    "                row = json.loads(line)\n",
    "                uid = row.get('uid')\n",
    "                records = row.get('records', [])\n",
    "                \n",
    "                # Flatten and normalize event records\n",
    "                flat_events = []\n",
    "                for event in records:\n",
    "                    props = event.get('properties') or {}\n",
    "                    flat_events.append({\n",
    "                        'date': event.get('date'),\n",
    "                        'event': event.get('event', ''),\n",
    "                        # Extract numeric properties with safe fallbacks\n",
    "                        'reward': props.get('reward', 0) if isinstance(props, dict) else 0,\n",
    "                        'package': props.get('package', 0) if isinstance(props, dict) else 0\n",
    "                    })\n",
    "\n",
    "                # Extract features and write to CSV\n",
    "                features = extract_features(flat_events, uid)\n",
    "                if features:\n",
    "                    # Initialize CSV writer with headers on first valid record\n",
    "                    if writer is None:\n",
    "                        writer = csv.DictWriter(outfile, fieldnames=list(features.keys()))\n",
    "                        writer.writeheader()\n",
    "                    writer.writerow(features)\n",
    "\n",
    "                # Print progress update every 10,000 records\n",
    "                if (i + 1) % 10000 == 0:\n",
    "                    print(f\"Processed {i+1} lines...\")\n",
    "\n",
    "                if (i + 1) % 5000 == 0:\n",
    "                    break\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping line {i} due to error: {e}\")\n",
    "                continue\n",
    "\n",
    "    print(f\"âœ… Finished writing features to {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000 lines...\n",
      "Processed 20000 lines...\n",
      "âœ… Finished writing features to data/game2_processed/train_features.csv\n"
     ]
    }
   ],
   "source": [
    "data = stream_features_to_csv_train(\"data/game2_processed/train.jsonl\", \"data/game2_processed/train_features.csv\")\n",
    "# ds1 = pd.DataFrame(\"data/game2_processed/train_features.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "ds1 = pd.read_csv(\"data/game2_processed/train_features.csv\")\n",
    "print(type(ds1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Finished writing features to data/game2_processed/test_features.csv\n"
     ]
    }
   ],
   "source": [
    "stream_features_to_csv_test(\"data/game2_processed/test.jsonl\", \"data/game2_processed/test_features.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2 = pd.read_csv(\"data/game2_processed/test_features.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Decision Tree...\n",
      "Training Random Forest...\n",
      "Training Logistic Regression...\n",
      "\n",
      "Decision Tree Feature Importance:\n",
      "days_since_last_play    0.648071\n",
      "play_count              0.223673\n",
      "active_days             0.048484\n",
      "avg_session_gap         0.030216\n",
      "score_std               0.019405\n",
      "best_score              0.017416\n",
      "mean_score              0.012735\n",
      "last_7_days_activity    0.000000\n",
      "dtype: float64\n",
      "\n",
      "Random Forest Feature Importance:\n",
      "active_days             0.291933\n",
      "days_since_last_play    0.288643\n",
      "play_count              0.174741\n",
      "avg_session_gap         0.097395\n",
      "best_score              0.064203\n",
      "score_std               0.046188\n",
      "mean_score              0.036897\n",
      "last_7_days_activity    0.000000\n",
      "dtype: float64\n",
      "\n",
      "Saved test predictions: ../data/game2_processed/ds2_with_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def prepare_model_data(ds1, ds2):\n",
    "    \"\"\"Prepare features and labels for modeling\"\"\"\n",
    "    # Features (exclude device ID and churn label)\n",
    "    feature_cols = [col for col in ds1.columns if col not in ['uid', 'churn']]\n",
    "    \n",
    "    # DS1 (Train)\n",
    "    X_train = ds1[feature_cols]\n",
    "    y_train = ds1['churn']\n",
    "    \n",
    "    # DS2 (Test) \n",
    "    X_test = ds2[feature_cols]\n",
    "    \n",
    "    # Impute missing values (using train stats)\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X_train = imputer.fit_transform(X_train)\n",
    "    X_test = imputer.transform(X_test)\n",
    "    \n",
    "    # Scale features (using train stats)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train, X_test, y_train, feature_cols\n",
    "\n",
    "def train_models(X_train, y_train):\n",
    "    \"\"\"Train multiple classifiers\"\"\"\n",
    "    models = {\n",
    "        'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42),\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42)\n",
    "    }\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"Training {name}...\")\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "    return models\n",
    "\n",
    "def evaluate_models(models, X_test, ds2, feature_cols):\n",
    "    \"\"\"Evaluate models on test data\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        # Get predicted probabilities\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "        ds2[f'{name}_churn_prob'] = y_proba\n",
    "        \n",
    "        # Feature importance (for tree models)\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importance = pd.Series(model.feature_importances_, index=feature_cols)\n",
    "            print(f\"\\n{name} Feature Importance:\")\n",
    "            print(importance.sort_values(ascending=False).head(10))\n",
    "        \n",
    "        results[name] = model\n",
    "    \n",
    "    # Save test results with predictions\n",
    "    ds2.to_csv(\"data/game2_processed/ds2_with_predictions.csv\", index=False)\n",
    "    print(\"\\nSaved test predictions: ../data/game2_processed/ds2_with_predictions.csv\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Prepare data\n",
    "X_train, X_test, y_train, feature_cols = prepare_model_data(ds1, ds2)\n",
    "\n",
    "# Train models\n",
    "models = train_models(X_train, y_train)\n",
    "\n",
    "# Evaluate on DS2\n",
    "model_results = evaluate_models(models, X_test, ds2, feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved DS1 (training) as JSONL: ../data/game2_processed/ds1_train.jsonl\n",
      "Saved DS2 (test) as JSONL: ../data/game2_processed/ds2_test.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Save datasets in JSONL format\n",
    "import jsonlines\n",
    "\n",
    "def save_to_jsonl(df, path):\n",
    "    with jsonlines.open(path, 'w') as writer:\n",
    "        writer.write_all(df.to_dict('records'))\n",
    "\n",
    "# Save DS1 (training data)\n",
    "save_to_jsonl(ds1, \"data/game2_processed/ds1_train.jsonl\")\n",
    "print(\"Saved DS1 (training) as JSONL: ../data/game2_processed/ds1_train.jsonl\")\n",
    "\n",
    "# Save DS2 (test data) - without churn labels if they don't exist\n",
    "if 'churned' in ds2.columns:\n",
    "    save_to_jsonl(ds2.drop(columns=['churned']), \"data/game2_processed/ds2_test.jsonl\")\n",
    "else:\n",
    "    save_to_jsonl(ds2, \"data/game2_processed/ds2_test.jsonl\")\n",
    "print(\"Saved DS2 (test) as JSONL: ../data/game2_processed/ds2_test.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load GPT-2 model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'gpt2' and tokenizer loaded successfully with pad token set.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_id = \"gpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "print(f\"Model '{model_id}' and tokenizer loaded successfully with pad token set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate prompts for LLM churn prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 5000 prompts.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "prompts = []\n",
    "\n",
    "for index, user_data in ds2.iterrows():\n",
    "    user_id = user_data['uid']\n",
    "    play_count = user_data.get('play_count', 'N/A')\n",
    "    active_days = user_data.get('active_days', 'N/A')\n",
    "    days_since_last_play = user_data.get('days_since_last_play', 'N/A')\n",
    "\n",
    "    prompt = (\n",
    "        f\"User ID: {user_id}\\n\"\n",
    "        f\"Play Count: {play_count}\\n\"\n",
    "        f\"Active Days: {active_days}\\n\"\n",
    "        f\"Days Since Last Play: {days_since_last_play}\\n\"\n",
    "        \"\\n\"\n",
    "        \"Based on this user's activity data, do you predict they will churn (stop playing)? Respond with either 'Churn' or 'Not Churn'.\"\n",
    "    )\n",
    "    prompts.append(prompt)\n",
    "\n",
    "print(f\"Generated {len(prompts)} prompts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate LLM churn predictions for a subset of users \n",
    "\n",
    "Note: for testing we only extracted 100 prompts\n",
    "\n",
    "to change this please alter this line \"for i, prompt in enumerate(prompts[:100]):\"\n",
    "\n",
    "with \"for i, prompt in enumerate(prompts):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions for 5000 users...\n",
      "\n",
      "Prediction generation complete.\n",
      "Generated predictions for 100 users.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "predictions = {}\n",
    "\n",
    "print(f\"Generating predictions for {len(prompts)} users...\")\n",
    "\n",
    "for i, prompt in enumerate(prompts[:100]):\n",
    "    try:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "\n",
    "        output_sequences = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=20,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "        generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "\n",
    "        prediction = \"Unknown\"\n",
    "        generated_text_lower = generated_text.lower()\n",
    "\n",
    "        if \"churn\" in generated_text_lower:\n",
    "            if \"not churn\" in generated_text_lower:\n",
    "                prediction = \"Not Churn\"\n",
    "            else:\n",
    "                 prediction = \"Churn\"\n",
    "\n",
    "        elif \"not churn\" in generated_text_lower:\n",
    "            prediction = \"Not Churn\"\n",
    "\n",
    "        user_id = prompt.split('\\n')[0].replace(\"User ID: \", \"\").strip()\n",
    "\n",
    "        predictions[user_id] = prediction\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing prompt {i}: {e}\")\n",
    "        try:\n",
    "            user_id = prompt.split('\\n')[0].replace(\"User ID: \", \"\").strip()\n",
    "            predictions[user_id] = \"Error\"\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "print(\"\\nPrediction generation complete.\")\n",
    "print(f\"Generated predictions for {len(predictions)} users.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Evaluate LLM churn predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating LLM Churn Predictions:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Churn       0.00      0.00      0.00        77\n",
      "   Not Churn       0.23      1.00      0.37        23\n",
      "\n",
      "    accuracy                           0.23       100\n",
      "   macro avg       0.12      0.50      0.19       100\n",
      "weighted avg       0.05      0.23      0.09       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "\n",
    "processed_user_ids = list(predictions.keys())\n",
    "\n",
    "actual_labels_df = ds2[ds2['uid'].astype(str).isin(processed_user_ids)].copy()\n",
    "actual_labels_df['actual_churn_label'] = actual_labels_df['churn'].apply(lambda x: 'Churn' if x else 'Not Churn')\n",
    "\n",
    "actual_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for index, row in actual_labels_df.iterrows():\n",
    "    user_id = str(row['uid'])\n",
    "    actual_label = row['actual_churn_label']\n",
    "\n",
    "    predicted_label = predictions.get(user_id, \"Unknown\")\n",
    "\n",
    "    actual_labels.append(actual_label)\n",
    "    predicted_labels.append(predicted_label)\n",
    "\n",
    "if not actual_labels:\n",
    "    print(\"No users processed for evaluation.\")\n",
    "else:\n",
    "    print(\"\\nEvaluating LLM Churn Predictions:\")\n",
    "\n",
    "    all_possible_labels = ['Churn', 'Not Churn'] + list(set(predicted_labels) - {'Churn', 'Not Churn'})\n",
    "\n",
    "    print(classification_report(actual_labels, predicted_labels, labels=['Churn', 'Not Churn'], zero_division=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
